name: ML CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-test-train:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout repository
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Run code quality checks
      - name: Run Ruff
        run: ruff check src tests

      - name: Run Flake8
        run: flake8 src tests

      - name: Run Black check
        run: black --check src tests

      # Step 5: Run tests
      - name: Run Pytest
        run: pytest --maxfail=1 --disable-warnings -q

      # Step 6: Run data validation, training, evaluation, and comparison
      - name: Run model training and comparison
        run: |
          python - <<'EOF'
          import pandas as pd
          from src.data_validation import validate_dataframe
          from src.train import train_model
          from src.evaluate import evaluate_model, compare_with_previous
          import joblib
          import os
          import json
          import shutil

          df = pd.DataFrame({
              "feature1": [1, 2, 3, 4, 5],
              "feature2": [10, 20, 30, 40, 50],
              "target": [0, 1, 0, 1, 0],
          })

          if not validate_dataframe(df):
              raise ValueError("Data validation failed")

          model, (X_val, y_val), model_path = train_model(df)
          metrics = evaluate_model(model, X_val, y_val)

          if not compare_with_previous():
              print("Rolling back to previous best model...")
              if os.path.exists(model_path):
                  os.remove(model_path)
              exit(1)

          print("Model accepted, accuracy:", metrics["accuracy"])
          EOF

      # Step 7: Upload best model and report artifacts
      - name: Upload artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: best-model-and-report
          path: |
            models/
            reports/previous_performance.json

      # Step 8: Create GitHub Release for best model
      - name: Create GitHub Release
        if: success()
        uses: softprops/action-gh-release@v2
        with:
          tag_name: v${{ github.run_number }}
          name: "Best Model v${{ github.run_number }}"
          body: "Auto-selected best model based on accuracy comparison."
          files: |
            models/*.joblib
            reports/previous_performance.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
