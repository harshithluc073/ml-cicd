name: ML CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-test-train:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout repository
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Run code quality checks
      - name: Run Ruff
        run: ruff check src tests

      - name: Run Flake8
        run: flake8 src tests

      - name: Run Black check
        run: black --check src tests

      # Step 5: Run tests
      - name: Run Pytest
        run: |
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          pytest --maxfail=1 --disable-warnings -q


      # Step 6: Run data validation, training, evaluation, and comparison
      - name: Run model training and comparison
        run: |
          python - <<'EOF'
          import pandas as pd
          from src.data_validation import validate_dataframe
          from src.train import train_model
          from src.evaluate import evaluate_model, compare_with_previous
          import joblib
          import os
          import json
          import shutil

          df = pd.DataFrame({
              "feature1": [1, 2, 3, 4, 5],
              "feature2": [10, 20, 30, 40, 50],
              "target": [0, 1, 0, 1, 0],
          })

          if not validate_dataframe(df):
              raise ValueError("Data validation failed")

          model, (X_val, y_val), model_path = train_model(df)
          metrics = evaluate_model(model, X_val, y_val)

          if not compare_with_previous():
              print("Rolling back to previous best model...")
              if os.path.exists(model_path):
                  os.remove(model_path)
              exit(1)

          print("Model accepted, accuracy:", metrics["accuracy"])
          EOF

      # Step 7: Upload best model and report artifacts
      - name: Upload artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: best-model-and-report
          path: |
            models/
            reports/previous_performance.json

      # Step 8: Create GitHub Release for best model
      - name: Create GitHub Release
        if: success()
        uses: softprops/action-gh-release@v2
        with:
          tag_name: v${{ github.run_number }}
          name: "Best Model v${{ github.run_number }}"
          body: "Auto-selected best model based on accuracy comparison."
          files: |
            models/*.joblib
            reports/previous_performance.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # Step 9: Notify in GitHub Discussions
      - name: Post release notification
        if: success()
        uses: peter-evans/create-or-update-discussion@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          repository: ${{ github.repository }}
          category: "Announcements"
          title: "Model v${{ github.run_number }} released"
          body: |
            âœ… **Model v${{ github.run_number }} Released Successfully**
            - Accuracy: See attached `previous_performance.json`
            - Artifacts: [View Release](${{ github.server_url }}/${{ github.repository }}/releases/tag/v${{ github.run_number }})
            - Triggered by: @${{ github.actor }}

      # Step 10: Update performance dashboard
      - name: Generate performance dashboard
        if: success()
        run: python src/dashboard.py

      - name: Upload dashboard artifact
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: model-performance-dashboard
          path: dashboard/performance.md


